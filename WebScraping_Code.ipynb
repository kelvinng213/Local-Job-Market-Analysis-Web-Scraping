{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-007e12dd7468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"FYwKg _3j_fQ _2PHih _3sxhA _1Epz5_6 _1A6vC_6 _20Cd9 _3RqUb_6 _3MPd_ Of_Sx_6 _2qcxd _3_RCw_6 C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 MxhoO_6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmaxpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"option\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "URL = \"https://hk.jobsdb.com/hk/search-jobs/data-analyst/\"\n",
    "URL1 = \"https://hk.jobsdb.com\"\n",
    "driver = webdriver.Chrome(executable_path='/Applications/chromedriver')\n",
    "driver.get(URL)\n",
    "wait = WebDriverWait(driver,10)\n",
    "subhtml = driver.page_source\n",
    "soup = BeautifulSoup(subhtml, \"html.parser\")\n",
    "x = soup.find_all(class_=\"FYwKg _3j_fQ _2PHih _3sxhA _1Epz5_6 _1A6vC_6 _20Cd9 _3RqUb_6 _3MPd_ Of_Sx_6 _2qcxd _3_RCw_6 C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 MxhoO_6\")\n",
    "maxpage = URL + (x[0].find_all(\"option\")[-1].text)\n",
    "print(maxpage)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "while True:\n",
    "    for i in soup.find_all(class_=\"FYwKg _3j_fQ _2mOt7_6 _1A6vC_6 _3VCZm _29sNS _2cWXo _1Swh0 _3gPuF_6 _2Nlzp_6\"):\n",
    "        try:\n",
    "            title = i.find(class_=\"FYwKg _2j8fZ_6 sIMFL_6 _1JtWu_6\").text\n",
    "    #         print(title)\n",
    "        except:\n",
    "            title = np.nan\n",
    "        try:\n",
    "            company = i.find(class_=\"ELZOd_6 qbDva _2CELK_6 FYwKg _2k9O2 _29sNS _58veS_6\").text\n",
    "    #         print(company)\n",
    "        except:\n",
    "            company = np.nan\n",
    "        posts = i.a[\"href\"]\n",
    "        if posts:\n",
    "            posts = posts\n",
    "            subhtml = driver.get(posts)\n",
    "            subhtml = driver.page_source\n",
    "            soup = BeautifulSoup(subhtml,\"html.parser\")\n",
    "        try:\n",
    "            new_a = soup.find_all(class_=\"FYwKg d7v3r _3122U_6\")\n",
    "            location_salary_postdate = ((new_a[0].text))\n",
    "    #         print(location_salary_postdate)\n",
    "        except:\n",
    "            location_salary_postdate = np.nan\n",
    "        try: \n",
    "            new_a = soup.find_all(class_=\"FYwKg _1GAuD C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 _2WTa0_6\") \n",
    "            job_highlights = (new_a[1].text+','+new_a[3].text+','+new_a[5].text)\n",
    "    #         print(job_highlights)\n",
    "        except:\n",
    "            job_highlights = np.nan\n",
    "        try:\n",
    "            new_a = soup.find(class_=\"vDEj0_6\")\n",
    "            job_description = new_a.find_all(\"ul\")[0].li.text\n",
    "            print(y.text)\n",
    "#             new_a = soup.find(class_=\"vDEj0_6\")\n",
    "#             job_description = new_a.div.text\n",
    "    #         print(job_description)\n",
    "        except:\n",
    "            newa = soup.find(class_=\"vDEj0_6\")\n",
    "            job_description = new_a.find_all(\"span\")[0].text\n",
    "#             job_description = np.nan\n",
    "        try:\n",
    "            new_a = soup.find_all(class_=\"FYwKg _32Ekc _2fqoM_6 _1hqiH_6\") \n",
    "            career_level = new_a[0].text[12:]\n",
    "    #         print(career_level)\n",
    "        except:\n",
    "            career_level = np.nan\n",
    "        try:\n",
    "            qualification = new_a[1].text[13:]\n",
    "    #         print(qualification)\n",
    "        except:\n",
    "            qualification = np.nan\n",
    "        try:\n",
    "            years_of_exp = new_a[2].text[19:]\n",
    "    #         print(years_of_exp)\n",
    "        except:\n",
    "            years_of_exp = np.nan\n",
    "        try:\n",
    "            job_type = new_a[3].text[8:]\n",
    "    #         print(job_type)\n",
    "        except:\n",
    "            job_type = np.nan\n",
    "        try:\n",
    "            new_a = soup.find_all(class_=\"FYwKg d7v3r _3BZ6E_6\") \n",
    "            industry_benefits_others = new_a[1].text\n",
    "#             print(industry_benefits_others)\n",
    "        except:\n",
    "            industry_benefits_others = np.nan\n",
    "        df = df.append({\"title\": title, \"company\": company, \"location_salary_postdate\": location_salary_postdate, \"highlights\": job_highlights, \"description\": job_description, \"career_level\": career_level, \"qualification\": qualification, \"years_of_exp\": years_of_exp, \"job_type\": job_type, \"industry_benefits_others\": industry_benefits_others,},ignore_index=True)\n",
    "    subhtml = driver.get(URL)\n",
    "    print(URL)\n",
    "    subhtml = driver.page_source\n",
    "    soup = BeautifulSoup(subhtml,\"html.parser\")\n",
    "    nextlink = soup.find_all(class_=\"FYwKg _2cWXo _1QYmq\")\n",
    "    if URL == maxpage:\n",
    "        break\n",
    "    else:\n",
    "        nextlink = URL1 + nextlink[0].find_all('a')[-1][\"href\"]\n",
    "        URL = nextlink\n",
    "        print(URL)\n",
    "        subhtml = requests.get(URL)\n",
    "        soup = BeautifulSoup(subhtml.text,\"html.parser\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hk.jobsdb.com/hk/search-jobs/data/9'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"https://hk.jobsdb.com/hk/search-jobs/data/\"\n",
    "URL1 = \"https://hk.jobsdb.com\"\n",
    "driver = webdriver.Chrome(executable_path='/Applications/chromedriver')\n",
    "driver.get(URL)\n",
    "wait = WebDriverWait(driver,10)\n",
    "subhtml = driver.page_source\n",
    "soup = BeautifulSoup(subhtml, \"html.parser\")\n",
    "\n",
    "x = soup.find_all(class_=\"FYwKg _3j_fQ _2PHih _3sxhA _1Epz5_6 _1A6vC_6 _20Cd9 _3RqUb_6 _3MPd_ Of_Sx_6 _2qcxd _3_RCw_6 C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 MxhoO_6\")\n",
    "# URL + max(x[0].text)\n",
    "# nextlink = soup.find_all(class_=\"FYwKg _2cWXo _1QYmq\")\n",
    "# nextlink = URL1 + nextlink[0].find_all('a')[-1][\"href\"]\n",
    "# print(nextlink)\n",
    "\n",
    "# URL = nextlink\n",
    "# subhtml = requests.get(nextlink)\n",
    "# soup = BeautifulSoup(subhtml.text,\"html.parser\")\n",
    "# a = soup.find_all(class_=\"FYwKg _3j_fQ _2mOt7_6 _1A6vC_6 _3VCZm _29sNS _2cWXo _1Swh0 _3gPuF_6 _2Nlzp_6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e00d091ce5a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"FYwKg _3j_fQ _2PHih _3sxhA _1Epz5_6 _1A6vC_6 _20Cd9 _3RqUb_6 _3MPd_ Of_Sx_6 _2qcxd _3_RCw_6 C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 MxhoO_6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmaxpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"option\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "URL = \"https://hk.jobsdb.com/hk/search-jobs/data-engineer/\"\n",
    "URL1 = \"https://hk.jobsdb.com\"\n",
    "driver = webdriver.Chrome(executable_path='/Applications/chromedriver')\n",
    "driver.get(URL)\n",
    "wait = WebDriverWait(driver,200)\n",
    "subhtml = driver.page_source\n",
    "soup = BeautifulSoup(subhtml, \"html.parser\")\n",
    "x = soup.find_all(class_=\"FYwKg _3j_fQ _2PHih _3sxhA _1Epz5_6 _1A6vC_6 _20Cd9 _3RqUb_6 _3MPd_ Of_Sx_6 _2qcxd _3_RCw_6 C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 MxhoO_6\")\n",
    "maxpage = URL + (x[0].find_all(\"option\")[-1].text)\n",
    "print(maxpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data_Analyst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'83'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = soup.find_all(class_=\"FYwKg _3j_fQ _2PHih _3sxhA _1Epz5_6 _1A6vC_6 _20Cd9 _3RqUb_6 _3MPd_ Of_Sx_6 _2qcxd _3_RCw_6 C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 MxhoO_6\")\n",
    "x[0].find_all(\"option\")[-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_a = soup.find_all(class_=\"FYwKg _1GAuD C6ZIU_6 _6ufcS_6 _27Shq_6 _29m7__6 _2WTa0_6\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
